{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bb5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate file rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 14:05:16.333073: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:453] fused(ShuffleDatasetV3:58,RepeatDataset:59): Filling up shuffle buffer (this may take a while): 757 of 4096\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Sequence\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# Config (edit to your needs)\n",
    "# ------------------------------\n",
    "SR = 22050\n",
    "NFFT = 1024\n",
    "HOP = 512\n",
    "NMELS = 128\n",
    "FMIN = 200.0\n",
    "FMAX = 8000.0\n",
    "\n",
    "FRAME_LENGTH = NFFT\n",
    "FRAME_STEP = HOP\n",
    "\n",
    "LOG_EPS = 1e-6\n",
    "\n",
    "# Different functions for each dataset\n",
    "def load_birdclef(audio_root, path, target, folds=5):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    df[\"path\"] = audio_root + \"/\" + df[\"primary_label\"] + \"/\" + df[\"filename\"]\n",
    "    \n",
    "    # Generate the binary labels\n",
    "    y = (df['primary_label'] == target).astype(int).values\n",
    "    \n",
    "    # Sanity check\n",
    "    dups = df['path'].duplicated().sum()  # or 'filename'\n",
    "    print(\"Duplicate file rows:\", dups)\n",
    "    \n",
    "    # Get k folds\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    for tr, va in skf.split(df, y):\n",
    "        splits.append({\n",
    "            \"train\":tr, \n",
    "            \"val\": va, \n",
    "            \"train_size\": len(tr),\n",
    "            \"val_size\": len(va),\n",
    "            \"train_pos_ratio\": y[tr].mean(),\n",
    "            \"val_pos_ratio\": y[va].mean(),\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    return df, splits\n",
    "\n",
    "################################\n",
    "# Data Augmentations #\n",
    "################################\n",
    "def aug_rms_dbfs(x):\n",
    "    rms = tf.sqrt(tf.reduce_mean(tf.square(x)) + 1e-12)\n",
    "    return 20.0 * tf.math.log(rms + 1e-12) / tf.math.log(10.0)\n",
    "\n",
    "def aug_rms_normalize(x, target_db=-20.0):\n",
    "    cur = aug_rms_dbfs(x)\n",
    "    gain_db = target_db - cur\n",
    "    gain = tf.pow(10.0, gain_db / 20.0)\n",
    "    y = x * gain\n",
    "    return tf.clip_by_value(y, -1.0, 1.0)\n",
    "\n",
    "def aug_volume_jitter(x, min_db=-6.0, max_db=6.0):\n",
    "    gain_db = tf.random.uniform([], min_db, max_db)\n",
    "    gain = tf.pow(10.0, gain_db / 20.0)\n",
    "    y = x * gain\n",
    "    return tf.clip_by_value(y, -1.0, 1.0)\n",
    "\n",
    "def aug_gaussian_noise_snr(x, snr_db=5.0):\n",
    "    # Scale Gaussian noise to achieve target SNR in dB\n",
    "    rms_sig = tf.sqrt(tf.reduce_mean(tf.square(x)) + 1e-12)\n",
    "    snr_lin = tf.pow(10.0, snr_db / 20.0)\n",
    "    rms_noise = rms_sig / snr_lin\n",
    "    noise = tf.random.normal(tf.shape(x))\n",
    "    noise_rms = tf.sqrt(tf.reduce_mean(tf.square(noise)) + 1e-12)\n",
    "    noise = noise * (rms_noise / (noise_rms + 1e-12))\n",
    "    y = x + noise\n",
    "    return tf.clip_by_value(y, -1.0, 1.0)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Helpers: audio decode + loudness + augment\n",
    "# ------------------------------\n",
    "\n",
    "def load_audio(path, sr=SR):\n",
    "    \"\"\"Return mono float32 waveform at target sr from any audio file (.ogg, .wav, .flac, ...).\"\"\"\n",
    "    def _read(p):\n",
    "        import soundfile as sf\n",
    "        import librosa\n",
    "        p = p.numpy().decode(\"utf-8\")\n",
    "        y, sr0 = sf.read(p, always_2d=False)       # y: [T] or [T, C], dtype float/PCM\n",
    "        if y.ndim == 2:                             # stereo -> mono\n",
    "            y = y.mean(axis=1)\n",
    "        if sr0 != sr:                               # resample if needed\n",
    "            y = librosa.resample(y.astype(np.float32), orig_sr=sr0, target_sr=sr)\n",
    "        y = y.astype(np.float32)\n",
    "        # clamp to [-1, 1] just in case\n",
    "        m = np.max(np.abs(y)) + 1e-9\n",
    "        if m > 1.0: y = (y / m).astype(np.float32)\n",
    "        return y\n",
    "    wav = tf.py_function(_read, [path], Tout=tf.float32)\n",
    "    wav.set_shape([None])  # 1-D waveform\n",
    "    return wav\n",
    "\n",
    "def build_file_lists(df, split, target):\n",
    "    # This function creates lists of filepaths for the split\n",
    "    # This results in 4 lists, train_pos, train_neg, val_pos and val_neg\n",
    "    tr_idx, va_idx = split[\"train\"], split[\"val\"]\n",
    "    train = df.iloc[tr_idx]; val = df.iloc[va_idx]\n",
    "    \n",
    "    pos_train = train[train.primary_label == target][\"path\"].tolist()\n",
    "    neg_train = train[train.primary_label != target][\"path\"].tolist()\n",
    "\n",
    "    pos_val = val[val.primary_label == target][\"path\"].tolist()\n",
    "    neg_val = val[val.primary_label != target][\"path\"].tolist()\n",
    "    return (pos_train, neg_train), (pos_val, neg_val)\n",
    "\n",
    "def plan_epoch(n_pos_train, pos_exposures=1.0, neg_ratio=1.0, batch_size=32, pos_ratio=0.5):\n",
    "    # This function plans the amount of positive and negative samples per training\n",
    "    # epoch. This ensures that we don't train on all the negatives and we can \n",
    "    # tune the amount of positives that we see\n",
    "    # n_pos_train: unique positive files in the train split\n",
    "    # pos_exposures: how many times you want to see each positive per epoch on average (1.0 â‰ˆ once)\n",
    "    # neg_ratio: negatives per positive overall (1.0 = 1:1, 2.0 = twice as many negs)\n",
    "    # pos_ratio: within-batch sampling weight for positives (0.5 = 50/50)\n",
    "    \n",
    "    P_pos = math.ceil(n_pos_train * pos_exposures)\n",
    "    # keep the global neg:pos = neg_ratio (independent of within-batch pos_ratio)\n",
    "    P_neg = math.ceil(P_pos * neg_ratio)\n",
    "\n",
    "    total_examples = P_pos + P_neg\n",
    "    steps_per_epoch = math.ceil(total_examples / batch_size)\n",
    "    return P_pos, P_neg, steps_per_epoch\n",
    "\n",
    "# ------------------------------\n",
    "# Feature extraction: power log-mel\n",
    "# ------------------------------\n",
    "def power_logmel(x, sr=SR, nfft=NFFT, hop=HOP, n_mels=NMELS, fmin=FMIN, fmax=FMAX):\n",
    "    # STFT -> power\n",
    "    stft = tf.signal.stft(x, frame_length=nfft, frame_step=hop, window_fn=tf.signal.hann_window, pad_end=True)\n",
    "    S = tf.abs(stft) ** 2  # [T_frames, nfft/2+1]\n",
    "    # Mel projection\n",
    "    mel_w = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins=n_mels,\n",
    "        num_spectrogram_bins=nfft // 2 + 1,\n",
    "        sample_rate=sr,\n",
    "        lower_edge_hertz=fmin,\n",
    "        upper_edge_hertz=fmax,\n",
    "    )\n",
    "    M = tf.matmul(S, mel_w)  # [T_frames, n_mels]\n",
    "    M = tf.transpose(M, [1, 0])  # [n_mels, time]\n",
    "    # Log of power mel\n",
    "    return tf.math.log(M + LOG_EPS)\n",
    "\n",
    "# ------------------------------\n",
    "# SpecAugment (optional) on mel features [n_mels, time]\n",
    "# ------------------------------\n",
    "def spec_augment(mel, num_time_masks=2, time_mask_width=16, num_freq_masks=2, freq_mask_width=8):\n",
    "    n_mels = tf.shape(mel)[0]\n",
    "    T = tf.shape(mel)[1]\n",
    "    m = tf.identity(mel)\n",
    "\n",
    "    def mask_freq(m):\n",
    "        w = tf.random.uniform([], 0, freq_mask_width + 1, dtype=tf.int32)\n",
    "        f0 = tf.random.uniform([], 0, tf.maximum(1, n_mels - w + 1), dtype=tf.int32)\n",
    "        mask = tf.concat([\n",
    "            tf.ones([f0, T], m.dtype),\n",
    "            tf.zeros([w, T], m.dtype),\n",
    "            tf.ones([n_mels - f0 - w, T], m.dtype)\n",
    "        ], axis=0)\n",
    "        return m * mask\n",
    "\n",
    "    def mask_time(m):\n",
    "        w = tf.random.uniform([], 0, time_mask_width + 1, dtype=tf.int32)\n",
    "        t0 = tf.random.uniform([], 0, tf.maximum(1, T - w + 1), dtype=tf.int32)\n",
    "        left = tf.ones([n_mels, t0], m.dtype)\n",
    "        mid  = tf.zeros([n_mels, w], m.dtype)\n",
    "        right= tf.ones([n_mels, T - t0 - w], m.dtype)\n",
    "        mask = tf.concat([left, mid, right], axis=1)\n",
    "        return m * mask\n",
    "\n",
    "    for _ in range(num_freq_masks):\n",
    "        m = mask_freq(m)\n",
    "    for _ in range(num_time_masks):\n",
    "        m = mask_time(m)\n",
    "    return m\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset builders\n",
    "# ------------------------------\n",
    "def make_waveform_ds(file_paths: Sequence[str], label: int, shuffle=True):\n",
    "    # Convert the raw audio into a tensorflow dataset based on the paths\n",
    "    ds = tf.data.Dataset.from_tensor_slices((list(file_paths), tf.fill([len(file_paths)], label)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(file_paths), reshuffle_each_iteration=True)\n",
    "    def _load(path, y):\n",
    "        wav = load_audio(path)\n",
    "        return wav, tf.cast(y, tf.float32)\n",
    "    return ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "def augment_waveform(wav, y, p_vol=0.9, p_noise=0.5, snr_low=-2.0, snr_high=8.0):\n",
    "    # Normalize first for stable SNR math\n",
    "    wav = aug_rms_normalize(wav, target_db=-20.0)\n",
    "    # Volume jitter\n",
    "    do_vol = tf.random.uniform([]) < p_vol\n",
    "    wav = tf.cond(do_vol, lambda: aug_volume_jitter(wav, -6.0, 6.0), lambda: wav)\n",
    "    # Gaussian SNR noise\n",
    "    do_noise = tf.random.uniform([]) < p_noise\n",
    "    snr = tf.random.uniform([], snr_low, snr_high)\n",
    "    wav = tf.cond(do_noise, lambda: aug_gaussian_noise_snr(wav, snr), lambda: wav)\n",
    "    return wav, y\n",
    "\n",
    "def to_logmel(wav, y):\n",
    "    mel = power_logmel(wav)   # [n_mels, time]\n",
    "    return mel, y\n",
    "\n",
    "def maybe_specaugment(mel, y, p=0.5):\n",
    "    return tf.cond(tf.random.uniform([]) < p,\n",
    "                   lambda: (spec_augment(mel), y),\n",
    "                   lambda: (mel, y))\n",
    "\n",
    "# ------------------------------\n",
    "# Balanced pipeline with sample_from_datasets + MixUp\n",
    "# ------------------------------\n",
    "def build_balanced_mel_pipeline_epoch(pos_files, neg_files, batch_size=32,\n",
    "                                      pos_ratio=0.5, apply_specaugment=True, mixup_alpha=0.3):\n",
    "    pos_ds = make_waveform_ds(pos_files, label=1).shuffle(4096).repeat()\n",
    "    neg_ds = make_waveform_ds(neg_files, label=0).shuffle(4096).repeat()\n",
    "\n",
    "    pos_ds = pos_ds.map(lambda x,y: augment_waveform(x,y, p_vol=0.9, p_noise=0.7), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    neg_ds = neg_ds.map(lambda x,y: augment_waveform(x,y, p_vol=0.5, p_noise=0.4), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    to_feat = lambda ds: ds.map(to_logmel, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    pos_ds, neg_ds = to_feat(pos_ds), to_feat(neg_ds)\n",
    "\n",
    "    if apply_specaugment:\n",
    "        pos_ds = pos_ds.map(maybe_specaugment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        neg_ds = neg_ds.map(maybe_specaugment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    ds = tf.data.Dataset.sample_from_datasets([pos_ds, neg_ds], weights=[pos_ratio, 1.0 - pos_ratio])\n",
    "    ds = ds.shuffle(2048).padded_batch(\n",
    "        batch_size,\n",
    "        padded_shapes=(tf.TensorShape([NMELS, None]), tf.TensorShape([]))\n",
    "    ).map(lambda m,y: (tf.ensure_shape(m, [None, NMELS, None]), tf.ensure_shape(y, [None])),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Batch-level MixUp (feature-space)\n",
    "    if mixup_alpha and mixup_alpha > 0:\n",
    "        def mixup_batch(mel_batch, y_batch):\n",
    "            # Random shuffle\n",
    "            indices = tf.random.shuffle(tf.range(tf.shape(mel_batch)[0]))\n",
    "            mel_shuf = tf.gather(mel_batch, indices)\n",
    "            y_shuf = tf.gather(y_batch, indices)\n",
    "            # Sample lambda\n",
    "            beta = tfp_distrib_beta(mixup_alpha, mixup_alpha, shape=[tf.shape(mel_batch)[0], 1, 1])\n",
    "            mel_mix = beta * mel_batch + (1. - beta) * mel_shuf\n",
    "            y_mix = tf.expand_dims(y_batch, -1) * beta[...,0] + tf.expand_dims(y_shuf, -1) * (1. - beta[...,0])\n",
    "            y_mix = tf.squeeze(y_mix, -1)\n",
    "            return mel_mix, y_mix\n",
    "\n",
    "        # simple Beta sampler without tfp dependency\n",
    "        def tfp_distrib_beta(a, b, shape):\n",
    "            # Gamma-based sampling: X~Gamma(a,1), Y~Gamma(b,1), X/(X+Y)\n",
    "            x = tf.random.gamma(shape, a, beta=1.0)\n",
    "            y = tf.random.gamma(shape, b, beta=1.0)\n",
    "            return x / (x + y + 1e-8)\n",
    "\n",
    "        ds = ds.map(mixup_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ------------------------------\n",
    "# Simple model + focal loss (optional)\n",
    "# ------------------------------\n",
    "def make_cnn(input_time=None):\n",
    "    inp = tf.keras.Input(shape=(NMELS, input_time), name=\"mel\")  # time can be variable (None)\n",
    "    x = tf.keras.layers.Permute((2,1))(inp)  # [B, T, n_mels]\n",
    "    x = tf.keras.layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    out = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return tf.keras.Model(inp, out)\n",
    "\n",
    "def binary_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
    "        pt = tf.where(tf.equal(y_true, 1.), y_pred, 1. - y_pred)\n",
    "        w = tf.where(tf.equal(y_true, 1.), alpha, 1. - alpha)\n",
    "        return tf.reduce_mean(-w * tf.pow(1. - pt, gamma) * tf.math.log(pt))\n",
    "    return loss\n",
    "\n",
    "# ------------------------------\n",
    "# Usage\n",
    "# ------------------------------\n",
    "# pos_files = [...]  # paths of target species files\n",
    "# neg_files = [...]  # paths of other/background files\n",
    "\n",
    "os.chdir(\"/home/joris/Thesis/new_attempt\")\n",
    "target = \"rucwar\"\n",
    "birdclef_df, splits = load_birdclef(\"datasets/birdclef_2021/train_short_audio\", \"datasets/birdclef_2021/train_metadata.csv\", target=target, folds=5)\n",
    "\n",
    "\n",
    "(pos_tr, neg_tr), (pos_va, neg_va) = build_file_lists(birdclef_df, splits[0], target)\n",
    "P_pos, P_neg, steps = plan_epoch(n_pos_train=len(pos_tr),\n",
    "                                 pos_exposures=1.0,   # see each positive ~once per epoch\n",
    "                                 neg_ratio=1.0,       # 1:1 overall\n",
    "                                 batch_size=32, pos_ratio=0.5)\n",
    "\n",
    "train_ds = build_balanced_mel_pipeline_epoch(pos_tr, neg_tr, batch_size=32, pos_ratio=0.5)\n",
    "val_ds   = build_balanced_mel_pipeline_epoch(pos_va, neg_va, batch_size=32, pos_ratio=0.5)\n",
    "\n",
    "for mel, label in train_ds.take(1):\n",
    "    print(\"mel shape:\", mel.shape)   # (batch_size, n_mels, T)\n",
    "    print(\"label shape:\", label.shape)\n",
    "# model = make_cnn(input_time=None)\n",
    "# model.compile(optimizer=\"adam\", loss=binary_focal_loss(gamma=2.0, alpha=0.25), metrics=[\"AUC\", \"Precision\", \"Recall\"])\n",
    "\n",
    "# model.fit(train_ds,\n",
    "#           steps_per_epoch=steps,\n",
    "#           validation_data=val_ds,\n",
    "#           validation_steps=max(1, len(pos_va+neg_va)//32),\n",
    "#           epochs=200)\n",
    "# model.fit(ds_train, epochs=20, steps_per_epoch=1000)  # steps_per_epoch because ds is infinite\n",
    "\n",
    "# ds_train = build_balanced_mel_pipeline(pos_files, neg_files, batch_size=32, apply_specaugment=True, mixup_alpha=0.3)\n",
    "# model = make_cnn(input_time=None)\n",
    "# model.compile(optimizer=\"adam\", loss=binary_focal_loss(gamma=2.0, alpha=0.25), metrics=[\"AUC\", \"Precision\", \"Recall\"])\n",
    "# model.fit(ds_train, epochs=20, steps_per_epoch=1000)  # steps_per_epoch because ds is infinite\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

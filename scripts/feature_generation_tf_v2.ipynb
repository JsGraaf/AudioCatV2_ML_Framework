{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d3c7635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "TF: 2.20.0\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorRt: 10.13.2.6\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_VLOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorrt as trt\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorRt:\", trt.__version__)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "random.seed(RANDOM_STATE)\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "TARGET = \"rucwar\"\n",
    "MIN_PER_CLASS = 10\n",
    "OUTER_SPLITS = 5\n",
    "INNER_SPLITS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_DURATION = 10 # seconds\n",
    "NEG_POS_RATIO = 3\n",
    "TRAIN_NEG_RATIO = 2\n",
    "VAL_NEG_RATIO = 2\n",
    "FILL_TYPE = \"pad\" # pad | tile\n",
    "N_FRAMES = 64\n",
    "FRAME_OVERLAP = int(np.ceil(N_FRAMES * 0.5))\n",
    "\n",
    "SPECTROGRAM_SECONDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3d8718ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate file rows: 0\n",
      "Total Positives: 154\n",
      "Total Negatives: 62695\n",
      "Avg samples per Class: 160\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# birdclef sanity checks\n",
    "def sanity_birdlcef(df: pd.DataFrame, target: str):\n",
    "    # File integrity\n",
    "    for r in df.itertuples():\n",
    "        if os.path.isfile(r.path) is not True:\n",
    "            print(f\"[ERROR]: {r.path} is not valid!\")\n",
    "    \n",
    "    # Dupicates\n",
    "    dups = df['path'].duplicated().sum()  # or 'filename'\n",
    "    print(\"Duplicate file rows:\", dups)\n",
    "    \n",
    "    # Count summary\n",
    "    print(f\"Total Positives: {df[df['primary_label'] == target].shape[0]}\")\n",
    "    print(f\"Total Negatives: {df[df['primary_label'] != target].shape[0]}\")\n",
    "    print(f\"Avg samples per Class: {df['primary_label'].value_counts().mean():.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Different functions for each dataset\n",
    "def load_birdclef(audio_root, path, target, min_per_class = MIN_PER_CLASS):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    df[\"path\"] = audio_root + \"/\" + df[\"primary_label\"] + \"/\" + df[\"filename\"]\n",
    "    \n",
    "    # Optional: drop rare classes (keeps CV stable)\n",
    "    if min_per_class > 1:\n",
    "        keep_labels = df[\"primary_label\"].value_counts()\n",
    "        keep_labels = keep_labels[keep_labels >= min_per_class].index\n",
    "        df = df[df[\"primary_label\"].isin(keep_labels)].reset_index(drop=True)\n",
    "    \n",
    "    # Perform sanity checks\n",
    "    sanity_birdlcef(df, target)\n",
    "\n",
    "    # Generate the binary labels, target = 1 else 0\n",
    "    labels = (df['primary_label'] == target).astype(int).values\n",
    "    \n",
    "    # Group based on auther + time te prevent straddeling\n",
    "    df[\"group_key\"] = df[\"author\"] + df[\"time\"]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "os.chdir(\"/home/joris/Thesis/new_attempt\")\n",
    "\n",
    "birdclef_df = load_birdclef(\"datasets/birdclef_2021/train_short_audio\", \"datasets/birdclef_2021/train_metadata.csv\", target=TARGET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00fe4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def make_nested_cv_splits(\n",
    "    df: pd.DataFrame,\n",
    "    target: str = TARGET,\n",
    "    outer_splits: int = OUTER_SPLITS,\n",
    "    inner_splits: int = INNER_SPLITS,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build nested CV folds for binary BirdCLEF: target species = 1, others = 0.\n",
    "    Uses StratifiedGroupKFold for both outer and inner splits to avoid leakage across groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Binary labels for stratification\n",
    "    y_all = (df[\"primary_label\"] == target).astype(int).values\n",
    "    groups_all = df[\"group_key\"].astype(str).fillna(\"NA\").values\n",
    "    idx_all = np.arange(len(df))\n",
    "\n",
    "    outer_kf = StratifiedGroupKFold(n_splits=outer_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    nested = []\n",
    "    for k, (outer_tr_idx, outer_te_idx) in enumerate(outer_kf.split(idx_all, y=y_all, groups=groups_all), start=1):\n",
    "        # Outer train/val pool and test set\n",
    "        trval_idx = idx_all[outer_tr_idx]\n",
    "        test_idx  = idx_all[outer_te_idx]\n",
    "\n",
    "        y_trval   = y_all[outer_tr_idx]\n",
    "        groups_trval = groups_all[outer_tr_idx]\n",
    "\n",
    "        # Inner CV on the outer train/val pool\n",
    "        inner_kf = StratifiedGroupKFold(n_splits=inner_splits, shuffle=True, random_state=random_state)\n",
    "        inner_folds = []\n",
    "        for j, (inner_tr_rel, inner_va_rel) in enumerate(inner_kf.split(trval_idx, y=y_trval, groups=groups_trval), start=1):\n",
    "            # Map relative indices back to global indices\n",
    "            inner_tr_idx = trval_idx[inner_tr_rel]\n",
    "            inner_va_idx = trval_idx[inner_va_rel]\n",
    "\n",
    "            inner_folds.append({\n",
    "                \"inner_fold\": j,\n",
    "                \"inner_train_idx\": inner_tr_idx,\n",
    "                \"inner_val_idx\":   inner_va_idx,\n",
    "            })\n",
    "\n",
    "        nested.append({\n",
    "            \"outer_fold\": k,\n",
    "            \"outer_train_idx\": trval_idx,\n",
    "            \"outer_test_idx\":  test_idx,\n",
    "            \"inner_folds\": inner_folds,\n",
    "            \"train_pos_ratio\": float(y_all[outer_tr_idx].mean()),\n",
    "            \"test_pos_ratio\":  float(y_all[outer_te_idx].mean()),\n",
    "        })\n",
    "\n",
    "    return nested\n",
    "\n",
    "    \n",
    "    \n",
    "cross_validation_sets = make_nested_cv_splits(birdclef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "512e7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio\n",
    "import subprocess\n",
    "\n",
    "def load_ogg_ffmpeg(path, sr=16000):\n",
    "    path = path.decode(\"utf-8\")\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-i\", path, \"-f\", \"f32le\",\n",
    "        \"-ac\", \"1\", \"-ar\", str(sr), \"pipe:1\", \"-loglevel\", \"quiet\"\n",
    "    ]\n",
    "    out = subprocess.check_output(cmd)\n",
    "    audio = np.frombuffer(out, np.float32)\n",
    "    return audio\n",
    "\n",
    "def load_ogg_librosa(path, sr=16000):\n",
    "    path = path.decode(\"utf-8\")\n",
    "    y, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    return y\n",
    "\n",
    "\n",
    "def audio_pipeline(filename, augment=False, gaussian=0):\n",
    "    # Load audio file as tensor \n",
    "    # audio_file = tf.numpy_function(load_ogg_ffmpeg, [filename, SAMPLE_RATE], tf.float32)\n",
    "    audio_file = tf.numpy_function(load_ogg_librosa, [filename, SAMPLE_RATE], tf.float32)\n",
    "    # audio_file, sr = librosa.load(filename.numpy(), sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Remove last dimension\n",
    "    waveform = audio_file[:SAMPLE_RATE * MAX_DURATION]\n",
    "    \n",
    "    if False:\n",
    "        # Trim the noise from the audio\n",
    "        position = tfio.audio.trim(waveform, axis=0, epsilon=0.1, name=\"Trim\")\n",
    "        \n",
    "        start = position[0]\n",
    "        stop = position[1]\n",
    "\n",
    "        processed = waveform[start:stop]\n",
    "    else:\n",
    "        processed = waveform[:SAMPLE_RATE * SPECTROGRAM_SECONDS]\n",
    "        \n",
    "    # if gaussian > 0:\n",
    "    #     processed = aug_gaussian_noise_snr(processed, gaussian)\n",
    "    \n",
    "    frame_count = tf.shape(processed)\n",
    "    # Calculate end padding\n",
    "    if (frame_count < SAMPLE_RATE):\n",
    "        processed = waveform\n",
    "        frame_count = tf.shape(processed)\n",
    "    \n",
    "    if FILL_TYPE == \"pad\":\n",
    "        padding = tf.maximum(0, (SAMPLE_RATE * SPECTROGRAM_SECONDS) - frame_count[0])\n",
    "        processed = tf.pad(processed, paddings=[[padding, 0]], name=\"Padding\")\n",
    "    elif FILL_TYPE == \"tile\":\n",
    "        repeats = tf.maximum(tf.cast(1, tf.int64), 1 + tf.cast(((SAMPLE_RATE * SPECTROGRAM_SECONDS) / frame_count), tf.int64))\n",
    "        \n",
    "        processed = tf.repeat(processed, repeats)\n",
    "        # Reduce size to max \n",
    "        processed = processed[:SAMPLE_RATE * SPECTROGRAM_SECONDS]\n",
    "        \n",
    "    if augment:\n",
    "        # Fade in and out\n",
    "        fade = tfio.audio.fade(\n",
    "            processed, fade_in=1000, fade_out=2000, mode=\"logarithmic\", name=\"Fade\")\n",
    "    else:\n",
    "        fade = processed\n",
    "    \n",
    "    # Band filter\n",
    "    from scipy import signal\n",
    "    b, a = signal.butter(4, [200, 7999], fs=SAMPLE_RATE, btype='band')\n",
    "    band_filter = tf.py_function(signal.lfilter, [b, a, fade], Tout=tf.float32, name=\"Filter\")\n",
    "\n",
    "    spectrogram = tfio.audio.spectrogram(band_filter, nfft=1024, window=512, stride=256)\n",
    "    mel_spectrogram = tfio.audio.melscale(spectrogram, rate=SAMPLE_RATE, mels=128, fmin=20, fmax=8000)\n",
    "    db_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "    \n",
    "    db_mel_spectrogram = tf.expand_dims(db_mel_spectrogram, -1)\n",
    "    \n",
    "    db_mel_spectrogram = tf.transpose(db_mel_spectrogram, perm=[1, 0 ,2])\n",
    "    \n",
    "    # db_mel_spectrogram = tf.ensure_shape(db_mel_spectrogram, (128, 188, 1))\n",
    "    \n",
    "    return db_mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ddbdd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# --------- Pick a Binary Focal Cross-Entropy loss safely ----------\n",
    "def get_binary_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    # 1) Native Keras (TF ≥ 2.12-ish)\n",
    "    if hasattr(tf.keras.losses, \"BinaryFocalCrossentropy\"):\n",
    "        return tf.keras.losses.BinaryFocalCrossentropy(gamma=gamma, alpha=alpha, from_logits=False)\n",
    "    # 2) TensorFlow Addons fallback\n",
    "    try:\n",
    "        import tensorflow_addons as tfa\n",
    "        return tfa.losses.SigmoidFocalCrossEntropy(gamma=gamma, alpha=alpha)\n",
    "    except Exception:\n",
    "        # 3) Minimal custom fallback (y_true∈{0,1}, y_pred∈[0,1])\n",
    "        def focal_bce(y_true, y_pred):\n",
    "            eps = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, eps, 1. - eps)\n",
    "            # standard BCE parts\n",
    "            ce_pos = -tf.math.log(y_pred)\n",
    "            ce_neg = -tf.math.log(1. - y_pred)\n",
    "            # focal weighting\n",
    "            loss_pos = alpha * tf.pow(1. - y_pred, gamma) * ce_pos * y_true\n",
    "            loss_neg = (1. - alpha) * tf.pow(y_pred, gamma) * ce_neg * (1. - y_true)\n",
    "            return tf.reduce_mean(loss_pos + loss_neg)\n",
    "        return focal_bce\n",
    "\n",
    "# --------- Simple binary CNN ----------\n",
    "def build_binary_cnn(\n",
    "    input_shape=(128, 64, 1),\n",
    "    lr=1e-3,\n",
    "    l2=1e-4,\n",
    "    dropout=0.25,\n",
    "    gamma=2.0,\n",
    "    alpha=0.25,\n",
    "):\n",
    "    \"\"\"\n",
    "    Binary classifier for log-mel spectrograms (target vs non-target).\n",
    "    Output: single sigmoid unit.\n",
    "    Loss: Binary Focal Cross-Entropy (with safe fallbacks).\n",
    "    \"\"\"\n",
    "    L2 = tf.keras.regularizers.l2(l2)\n",
    "    Conv = tf.keras.layers.Conv2D\n",
    "    focal_loss = get_binary_focal_loss(gamma=gamma, alpha=alpha)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv(32, (3, 3), padding=\"same\", kernel_regularizer=L2)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = Conv(32, (3, 3), padding=\"same\", kernel_regularizer=L2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)   # 128x64 -> 64x32\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv(64, (3, 3), padding=\"same\", kernel_regularizer=L2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = Conv(64, (3, 3), padding=\"same\", kernel_regularizer=L2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)   # 64x32 -> 32x16\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv(96, (3, 3), padding=\"same\", kernel_regularizer=L2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = Conv(96, (3, 3), padding=\"same\", kernel_regularizer=L2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)   # 32x16 -> 16x8\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    # Head\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=L2)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)  # binary\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=focal_loss,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(name=\"acc\", threshold=0.5),\n",
    "            tf.keras.metrics.AUC(curve=\"ROC\", name=\"auc\"),\n",
    "            tf.keras.metrics.AUC(curve=\"PR\",  name=\"auprc\"),\n",
    "            tf.keras.metrics.Precision(name=\"precision\", thresholds=0.5),\n",
    "            tf.keras.metrics.Recall(name=\"recall\", thresholds=0.5),\n",
    "        ],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bc68817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Sequence, Optional, Tuple\n",
    "\n",
    "def _map_pos(x, y):\n",
    "    return audio_pipeline(x, augment=True), y\n",
    "\n",
    "def _map_neg(x, y):\n",
    "    return audio_pipeline(x, augment=True), y\n",
    "\n",
    "def plan_epoch_counts(n_pos_train: int, neg_pos_ratio: float = 2.0) -> Tuple[int, int]:\n",
    "    P_pos = int(n_pos_train)               # see each positive once per epoch\n",
    "    P_neg = int(round(neg_pos_ratio * P_pos))\n",
    "    return P_pos, P_neg \n",
    "\n",
    "def sample_train_negatives(neg_all: Sequence[str], n_neg: int, seed: Optional[int] = None) -> Sequence[str]:\n",
    "    n = min(n_neg, len(neg_all))\n",
    "    rng = random.Random(seed)\n",
    "    return rng.sample(list(neg_all), n) if n > 0 else []\n",
    "\n",
    "def make_fixed_val_negatives(neg_all: Sequence[str], n_pos_val: int, neg_pos_ratio: int = NEG_POS_RATIO, seed: int = RANDOM_STATE) -> Sequence[str]:\n",
    "    n_neg = min(neg_pos_ratio * n_pos_val, len(neg_all))\n",
    "    rng = random.Random(seed)\n",
    "    return rng.sample(list(neg_all), n_neg) if n_neg > 0 else []\n",
    "\n",
    "def build_train_dataset(pos_files: Sequence[str], neg_files: Sequence[str], batch_size: int = BATCH_SIZE, shuffle: bool = True) -> tf.data.Dataset:\n",
    "    labels_pos = tf.ones([len(pos_files)], dtype=tf.float32)\n",
    "    labels_neg = tf.zeros([len(neg_files)], dtype=tf.float32)\n",
    "    \n",
    "    ds_pos = tf.data.Dataset.from_tensor_slices((list(pos_files), labels_pos)).map(_map_pos, num_parallel_calls=10)\n",
    "    ds_neg = tf.data.Dataset.from_tensor_slices((list(neg_files), labels_neg)).map(_map_neg, num_parallel_calls=10)\n",
    "\n",
    "    ds = ds_pos.concatenate(ds_neg)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(pos_files) + len(neg_files), seed=RANDOM_STATE, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).cache()\n",
    "    return ds\n",
    "\n",
    "\n",
    "def build_val_dataset(pos_files: Sequence[str], neg_files_fixed: Sequence[str], batch_size: int = BATCH_SIZE) -> tf.data.Dataset:\n",
    "    y_pos = tf.ones([len(pos_files)],        dtype=tf.float32)\n",
    "    y_neg = tf.zeros([len(neg_files_fixed)], dtype=tf.float32)\n",
    "\n",
    "    ds_pos = tf.data.Dataset.from_tensor_slices((list(pos_files), y_pos)).map(lambda x,y: (audio_pipeline(x, augment=False), y), num_parallel_calls=10)\n",
    "    ds_neg = tf.data.Dataset.from_tensor_slices((list(neg_files_fixed), y_neg)).map(lambda x,y: (audio_pipeline(x, augment=False), y), num_parallel_calls=10)\n",
    "\n",
    "    return ds_pos.concatenate(ds_neg).batch(batch_size).cache()\n",
    "\n",
    "def build_file_lists(df: pd.DataFrame, idx, target=TARGET):\n",
    "    sub = df.iloc[idx]\n",
    "    pos = sub[sub.primary_label == target][\"path\"].tolist()\n",
    "    neg = sub[sub.primary_label != target][\"path\"].tolist()\n",
    "    return pos, neg\n",
    "\n",
    "def make_epoch_train_dataset(pos_tr_all: Sequence[str], neg_tr_all: Sequence[str],\n",
    "                             neg_pos_ratio: float = 2.0, batch_size: int = BATCH_SIZE, seed: Optional[int] = RANDOM_STATE) -> tf.data.Dataset:\n",
    "    P_pos, P_neg = plan_epoch_counts(len(pos_tr_all), neg_pos_ratio)\n",
    "    neg_epoch = sample_train_negatives(neg_tr_all, P_neg, seed=seed)\n",
    "    return build_train_dataset(pos_tr_all, neg_epoch, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def make_fixed_val_dataset(pos_va_all: Sequence[str], neg_va_all: Sequence[str],\n",
    "                           neg_pos_ratio: int = 3, batch_size: int = BATCH_SIZE, seed: int = RANDOM_STATE) -> tf.data.Dataset:\n",
    "    neg_fixed = make_fixed_val_negatives(neg_va_all, len(pos_va_all), neg_pos_ratio=neg_pos_ratio, seed=seed)\n",
    "    return build_val_dataset(pos_va_all, neg_fixed, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def build_all_file_lists(df: pd.DataFrame, folds: dict, epoch: int = 0):\n",
    "    out = []\n",
    "    for fold in folds:\n",
    "        outer_fold = int(fold[\"outer_fold\"])\n",
    "        outer_train_idx = fold[\"outer_train_idx\"]\n",
    "        outer_test_idx = fold[\"outer_test_idx\"]\n",
    "    \n",
    "        # Build the file lists for the outer fold\n",
    "        pos_tr_all, neg_tr_all = build_file_lists(df, outer_train_idx)    \n",
    "        pos_test_all, neg_test_all = build_file_lists(df, outer_test_idx)\n",
    "    \n",
    "\n",
    "        # Rotate negatives each epoch via seed that depends on (outer, inner, epoch)\n",
    "        outer_train_ds = make_epoch_train_dataset(\n",
    "            pos_tr_all,\n",
    "            neg_tr_all,\n",
    "            neg_pos_ratio=TRAIN_NEG_RATIO,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=outer_fold,\n",
    "        )\n",
    "\n",
    "        outer_test_ds = make_fixed_val_dataset(\n",
    "            pos_test_all,\n",
    "            neg_test_all,\n",
    "            neg_pos_ratio=VAL_NEG_RATIO,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            seed=outer_fold,  # fixed per outer fold\n",
    "        )\n",
    "    \n",
    "        \n",
    "        inner_list = []\n",
    "        for inner in fold[\"inner_folds\"]:\n",
    "            inner_fold = int(inner[\"inner_fold\"])\n",
    "            inner_train_idx = inner[\"inner_train_idx\"]\n",
    "            inner_val_idx   = inner[\"inner_val_idx\"]\n",
    "\n",
    "            pos_tr_all, neg_tr_all = build_file_lists(df, inner_train_idx)\n",
    "            pos_va_all, neg_va_all = build_file_lists(df, inner_val_idx)\n",
    "\n",
    "            # Rotate negatives each epoch via seed that depends on (outer, inner, epoch)\n",
    "            train_ds = make_epoch_train_dataset(\n",
    "                pos_tr_all,\n",
    "                neg_tr_all,\n",
    "                neg_pos_ratio=TRAIN_NEG_RATIO,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                seed=outer_fold * 100_000 + inner_fold * 1_000 + epoch,\n",
    "            )\n",
    "\n",
    "            # Fixed validation and test datasets (reproducible seeds per outer/inner)\n",
    "            val_ds = make_fixed_val_dataset(\n",
    "                pos_va_all,\n",
    "                neg_va_all,\n",
    "                neg_pos_ratio=VAL_NEG_RATIO,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                seed=outer_fold * 100_000 + inner_fold,\n",
    "            )\n",
    "\n",
    "            test_ds = make_fixed_val_dataset(\n",
    "                pos_test_all,\n",
    "                neg_test_all,\n",
    "                neg_pos_ratio=VAL_NEG_RATIO,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                seed=outer_fold,  # fixed per outer fold\n",
    "            )\n",
    "\n",
    "            inner_list.append({\n",
    "                \"inner_fold\": inner_fold,\n",
    "                \"train_ds\": train_ds,\n",
    "                \"val_ds\":   val_ds,\n",
    "                \"test_ds\":  test_ds,\n",
    "            })\n",
    "        out.append({\"outer_fold\": outer_fold, \"train_ds\":outer_train_ds, \"test_ds\":outer_test_ds, \"inner\": inner_list})\n",
    "    return out\n",
    "        \n",
    "\n",
    "datasets = build_all_file_lists(birdclef_df, cross_validation_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f923103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "/home/joris/.pyenv/versions/3.12.11/lib/python3.12/site-packages/tensorboard/default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
       "  import pkg_resources\n",
       "2025-08-29 11:19:10.158198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
       "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
       "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
       "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
       "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
       "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
       "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
       "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
       "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
       "                   [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--detect_file_replacement BOOL]\n",
       "                   [--master_tpu_unsecure_channel ADDR]\n",
       "                   {serve} ...\n",
       "tensorboard: error: argument --logdir: expected one argument"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to start profiler: Another profiler is running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Running outer fold: 1\n",
      "[INFO]: Running inner fold: 1\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to start profiler: Another profiler is running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - acc: 0.5913 - auc: 0.4627 - auprc: 0.3183 - loss: 0.2604 - precision: 0.3208 - recall: 0.2024 - val_acc: 0.3333 - val_auc: 0.5206 - val_auprc: 0.3897 - val_loss: 0.7958 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - acc: 0.6627 - auc: 0.5477 - auprc: 0.4164 - loss: 0.2175 - precision: 0.4909 - recall: 0.3214 - val_acc: 0.6190 - val_auc: 0.4835 - val_auprc: 0.3587 - val_loss: 0.2841 - val_precision: 0.2727 - val_recall: 0.0857\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - acc: 0.6786 - auc: 0.6195 - auprc: 0.4395 - loss: 0.2063 - precision: 0.5484 - recall: 0.2024 - val_acc: 0.3333 - val_auc: 0.5106 - val_auprc: 0.3690 - val_loss: 1.9299 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - acc: 0.6865 - auc: 0.6519 - auprc: 0.5418 - loss: 0.1960 - precision: 0.5510 - recall: 0.3214 - val_acc: 0.3333 - val_auc: 0.5090 - val_auprc: 0.3346 - val_loss: 0.3634 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - acc: 0.7103 - auc: 0.6680 - auprc: 0.5473 - loss: 0.1913 - precision: 0.6486 - recall: 0.2857 - val_acc: 0.3333 - val_auc: 0.5186 - val_auprc: 0.3540 - val_loss: 0.6704 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 289ms/step - acc: 0.3333 - auc: 0.5422 - auprc: 0.3804 - loss: 0.6828 - precision: 0.3333 - recall: 1.0000\n",
      "[INFO]: Running inner fold: 2\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to start profiler: Another profiler is running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 898ms/step - acc: 0.5896 - auc: 0.5325 - auprc: 0.4287 - loss: 0.4221 - precision: 0.3551 - recall: 0.2106      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joris/.pyenv/versions/3.12.11/lib/python3.12/site-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - acc: 0.5424 - auc: 0.5269 - auprc: 0.4030 - loss: 0.3000 - precision: 0.3226 - recall: 0.3390 - val_acc: 0.6667 - val_auc: 0.3360 - val_auprc: 0.2677 - val_loss: 0.4604 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - acc: 0.5537 - auc: 0.5045 - auprc: 0.3457 - loss: 0.1584 - precision: 0.3571 - recall: 0.4237 - val_acc: 0.3333 - val_auc: 0.6529 - val_auprc: 0.4539 - val_loss: 0.5453 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - acc: 0.6497 - auc: 0.5444 - auprc: 0.3856 - loss: 0.1482 - precision: 0.4545 - recall: 0.2542 - val_acc: 0.6667 - val_auc: 0.3356 - val_auprc: 0.2633 - val_loss: 0.6194 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - acc: 0.6497 - auc: 0.6337 - auprc: 0.4260 - loss: 0.1332 - precision: 0.4681 - recall: 0.3729 - val_acc: 0.6667 - val_auc: 0.3350 - val_auprc: 0.2669 - val_loss: 0.4971 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - acc: 0.5198 - auc: 0.4199 - auprc: 0.2809 - loss: 0.1426 - precision: 0.2174 - recall: 0.1695 - val_acc: 0.3333 - val_auc: 0.6488 - val_auprc: 0.4501 - val_loss: 0.3154 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 623ms/step - acc: 0.3333 - auc: 0.5578 - auprc: 0.3967 - loss: 0.3395 - precision: 0.3333 - recall: 1.0000\n",
      "[INFO]: Running inner fold: 3\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to start profiler: Another profiler is running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 580ms/step - acc: 0.5781 - auc: 0.5130 - auprc: 0.3373 - loss: 0.1244 - precision: 0.3556 - recall: 0.3902 - val_acc: 0.6667 - val_auc: 0.3507 - val_auprc: 0.2650 - val_loss: 0.0998 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - acc: 0.4828 - auc: 0.5409 - auprc: 0.4463 - loss: 0.0938 - precision: 0.4000 - recall: 0.3077 - val_acc: 0.6667 - val_auc: 0.3607 - val_auprc: 0.3017 - val_loss: 0.0912 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - acc: 0.6133 - auc: 0.5506 - auprc: 0.3667 - loss: 0.0928 - precision: 0.3924 - recall: 0.3780 - val_acc: 0.3333 - val_auc: 0.6628 - val_auprc: 0.5204 - val_loss: 0.1440 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - acc: 0.5172 - auc: 0.5264 - auprc: 0.4644 - loss: 0.1035 - precision: 0.4286 - recall: 0.2308 - val_acc: 0.3333 - val_auc: 0.6758 - val_auprc: 0.5254 - val_loss: 0.1747 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - acc: 0.6367 - auc: 0.6143 - auprc: 0.4115 - loss: 0.0865 - precision: 0.4127 - recall: 0.3171 - val_acc: 0.3333 - val_auc: 0.7122 - val_auprc: 0.5920 - val_loss: 0.1946 - val_precision: 0.3333 - val_recall: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 614ms/step - acc: 0.3333 - auc: 0.5690 - auprc: 0.4176 - loss: 0.1979 - precision: 0.3333 - recall: 1.0000\n",
      "Inner CV mean test acc: 0.333\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Another profiler is running.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAlreadyExistsError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     45\u001b[39m best_inner = np.argmax(inner_scores)\n\u001b[32m     47\u001b[39m model = build_binary_cnn(input_shape=(\u001b[32m128\u001b[39m, \u001b[32m188\u001b[39m, \u001b[32m1\u001b[39m), \n\u001b[32m     48\u001b[39m                             lr=hparams_per_fold[best_inner][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m     49\u001b[39m                             gamma=hparams_per_fold[best_inner][\u001b[33m\"\u001b[39m\u001b[33mgamma\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m     50\u001b[39m                             alpha=hparams_per_fold[best_inner][\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperimental\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogs/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m model.fit(\n\u001b[32m     53\u001b[39m     outer_fold[\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m][best_inner][\u001b[33m\"\u001b[39m\u001b[33mtrain_ds\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     54\u001b[39m     epochs = EPOCHS_PER_FOLD,\n\u001b[32m     55\u001b[39m     validation_data = outer_fold[\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m][best_inner][\u001b[33m\"\u001b[39m\u001b[33mval_ds\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     56\u001b[39m     callbacks=[tboard_callback]\n\u001b[32m     57\u001b[39m )\n\u001b[32m     58\u001b[39m tf.profiler.experimental.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.11/lib/python3.12/site-packages/tensorflow/python/profiler/profiler_v2.py:110\u001b[39m, in \u001b[36mstart\u001b[39m\u001b[34m(logdir, options)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _profiler_lock:\n\u001b[32m    109\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m _profiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors.AlreadyExistsError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    111\u001b[39m                                     \u001b[33m'\u001b[39m\u001b[33mAnother profiler is running.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    112\u001b[39m   _profiler = _pywrap_profiler.ProfilerSession()\n\u001b[32m    113\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# support for namedtuple in pybind11 is missing, we change it to\u001b[39;00m\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# dict type first.\u001b[39;00m\n",
      "\u001b[31mAlreadyExistsError\u001b[39m: Another profiler is running."
     ]
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --bind_all --logdir\n",
    "\n",
    "per_fold_results = []\n",
    "\n",
    "EPOCHS_PER_FOLD = 5\n",
    "\n",
    "hparams_per_fold = {\n",
    "    0: {\"lr\":1e-3, \"gamma\":1.0, \"alpha\":0.25},\n",
    "    1: {\"lr\":1e-3, \"gamma\":2.0, \"alpha\":0.5},\n",
    "    2: {\"lr\":1e-3, \"gamma\":3.0, \"alpha\":0.75},\n",
    "    3: {\"lr\":1e-3, \"gamma\":4.0, \"alpha\":0.25},\n",
    "    4: {\"lr\":1e-3, \"gamma\":5.0, \"alpha\":0.25},\n",
    "}\n",
    "\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = \"logs/\",\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '1, 4')\n",
    "\n",
    "for outer_fold in datasets:\n",
    "    print(f\"[INFO]: Running outer fold: {outer_fold[\"outer_fold\"]}\")\n",
    "    inner_scores = []\n",
    "    for inner_fold in outer_fold[\"inner\"]:\n",
    "        print(f\"[INFO]: Running inner fold: {inner_fold[\"inner_fold\"]}\")\n",
    "        model = build_binary_cnn(input_shape=(128, 188, 1), \n",
    "                                 lr=hparams_per_fold[inner_fold[\"inner_fold\"]][\"lr\"], \n",
    "                                 gamma=hparams_per_fold[inner_fold[\"inner_fold\"]][\"gamma\"], \n",
    "                                 alpha=hparams_per_fold[inner_fold[\"inner_fold\"]][\"alpha\"])\n",
    "\n",
    "        model.fit(\n",
    "            inner_fold[\"train_ds\"],\n",
    "            epochs = EPOCHS_PER_FOLD,\n",
    "            validation_data=inner_fold[\"val_ds\"],\n",
    "            callbacks=[tboard_callback],\n",
    "            steps_per_epoch=8\n",
    "        )\n",
    "        \n",
    "        vals = model.evaluate(inner_fold[\"test_ds\"])\n",
    "        inner_scores.append(vals[1])\n",
    "    \n",
    "    mean_test_acc = np.mean(inner_scores)\n",
    "    print(f\"Inner CV mean test acc: {mean_test_acc:.3f}\")\n",
    "    \n",
    "    # Retrain with the best parameters\n",
    "    best_inner = np.argmax(inner_scores)\n",
    "    \n",
    "    model = build_binary_cnn(input_shape=(128, 188, 1), \n",
    "                                lr=hparams_per_fold[best_inner][\"lr\"], \n",
    "                                gamma=hparams_per_fold[best_inner][\"gamma\"], \n",
    "                                alpha=hparams_per_fold[best_inner][\"alpha\"])\n",
    "    tf.profiler.experimental.start('logs/')\n",
    "    model.fit(\n",
    "        outer_fold[\"inner\"][best_inner][\"train_ds\"],\n",
    "        epochs = EPOCHS_PER_FOLD,\n",
    "        validation_data = outer_fold[\"inner\"][best_inner][\"val_ds\"],\n",
    "        callbacks=[tboard_callback]\n",
    "    )\n",
    "    tf.profiler.experimental.stop()\n",
    "    vals = model.evaluate(outer_fold[\"test_ds\"])\n",
    "    per_fold_results.append(vals[1])\n",
    "\n",
    "print(per_fold_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
